{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ1ihxrmCcOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a933d76c-dc5e-40ee-dc22-79f6f593698f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "!pip install keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdIrUWdChKgR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import io\n",
        "import imageio\n",
        "from IPython.display import Image, display\n",
        "from ipywidgets import widgets, Layout, HBox"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B&W the images"
      ],
      "metadata": {
        "id": "t1g0vICh9L0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Directory containing the colored images\n",
        "input_directory = '/content/drive/MyDrive/pix2pix/1080/Modified_Dataset'  # Replace with the path to your colored images directory\n",
        "output_directory = '/content/drive/MyDrive/pix2pix/1080/Dataset_1080'  # Replace with the path to the directory to save the binary images\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Loop through each image in the input directory\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):  # Check if it's an image file\n",
        "        # Load the colored image\n",
        "        image_path = os.path.join(input_directory, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Convert the image to grayscale\n",
        "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Apply binary thresholding to create a black and white image\n",
        "        _, binary_image = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        # Save the binary image to the output directory with the same filename\n",
        "        output_path = os.path.join(output_directory, filename)\n",
        "        cv2.imwrite(output_path, binary_image)\n",
        "'''"
      ],
      "metadata": {
        "id": "wkLKDsaZ981X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert images into 25 batches\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "1kVrSfmWI7hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Directory containing the 500 images\n",
        "input_directory = '/content/drive/MyDrive/pix2pix/128/Dataset_128'  # Replace with your actual directory\n",
        "\n",
        "# Directory to save the batches of images\n",
        "output_directory = '/content/drive/MyDrive/pix2pix/128/batch_128'  # Replace with your desired output directory\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# List all image files in the input directory\n",
        "image_files = [f for f in os.listdir(input_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "# Set the batch size\n",
        "batch_size = 20\n",
        "\n",
        "# Loop through the image files and create batches\n",
        "for i in range(0, len(image_files), batch_size):\n",
        "    batch_images = image_files[i:i + batch_size]\n",
        "    batch_directory = os.path.join(output_directory, f'batch_{i // batch_size + 1}')\n",
        "    os.makedirs(batch_directory, exist_ok=True)\n",
        "\n",
        "    # Copy or move the images to the batch directory\n",
        "    for image_file in batch_images:\n",
        "        image_path = os.path.join(input_directory, image_file)\n",
        "        destination_path = os.path.join(batch_directory, image_file)\n",
        "        os.replace(image_path, destination_path)\n",
        "\n",
        "print(f'Converted {len(image_files)} images into {len(os.listdir(output_directory))} batches.')\n",
        "'''"
      ],
      "metadata": {
        "id": "ffnVFa4fIafi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Batches into npy"
      ],
      "metadata": {
        "id": "Zllw-TEWJQnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Directory containing 20 batches of grayscale images\n",
        "base_dir = '/content/drive/MyDrive/pix2pix/128/batch_128'\n",
        "\n",
        "# Directory to save the 'npy' files\n",
        "output_dir = '/content/drive/MyDrive/pix2pix/128/npy_128'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Function to get a list of all image files in a directory and its subdirectories\n",
        "def get_image_paths(directory):\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "    return image_paths\n",
        "\n",
        "# Loop through the 20 batches\n",
        "for batch_num in range(1, 26):\n",
        "    batch_dir = os.path.join(base_dir, f'batch_{batch_num}')\n",
        "\n",
        "    # Get a list of image file paths in the current batch directory\n",
        "    image_paths = get_image_paths(batch_dir)\n",
        "\n",
        "    # Create an empty list to store image data for this batch\n",
        "    image_data = []\n",
        "\n",
        "    # Loop through the image paths and load each image, then append it to the list\n",
        "    for img_path in image_paths:\n",
        "        img = Image.open(img_path)\n",
        "        #img = img.convert('L')  # Convert to grayscale if not already\n",
        "        img_array = np.array(img)\n",
        "        image_data.append(img_array)\n",
        "\n",
        "    # Convert the list of grayscale images to a NumPy array\n",
        "    image_data = np.array(image_data)\n",
        "\n",
        "    # Ensure the shape of the array is (num_images, height, width, 1)\n",
        "    image_data = image_data.reshape(image_data.shape[0], image_data.shape[1], image_data.shape[2], 1)\n",
        "\n",
        "    # Define the path for the output 'npy' file for this batch\n",
        "    output_npy_file = os.path.join(output_dir, f'batch_{batch_num}.npy')\n",
        "\n",
        "    # Save the NumPy array to an 'npy' file for this batch\n",
        "    np.save(output_npy_file, image_data)\n",
        "\n",
        "    # Optionally, you can load the 'npy' file back into a variable\n",
        "    loaded_image_data = np.load(output_npy_file)\n",
        "\n",
        "    # Check the shape of the loaded array\n",
        "    print(f'Batch {batch_num} Shape: {loaded_image_data.shape}')\n",
        "'''"
      ],
      "metadata": {
        "id": "HjhJ_ezO6Rs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/pix2pix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kuRkTTL6uhC",
        "outputId": "c20d0c87-0c58-4a1d-d291-49b7bb6e2106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/pix2pix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert 25 npy files into a single npy file"
      ],
      "metadata": {
        "id": "Id_-q_rLKq-6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg-4vHuGCxj-"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "# change the space accordingly\n",
        "f1 = np.zeros((25,20,128,128,1))\n",
        "# Directory containing the batch .npy files\n",
        "batch_npy_directory = '/content/drive/MyDrive/pix2pix/128/npy_128'  # Replace with the directory containing the batch .npy files\n",
        "\n",
        "# Directory to save the combined array\n",
        "output_directory = '/content/drive/MyDrive/pix2pix/128/npy_128'  # Replace with your desired output directory\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# List all .npy files in the batch directory\n",
        "npy_files = [f for f in os.listdir(batch_npy_directory) if f.endswith('.npy')]\n",
        "\n",
        "# Load and concatenate the batch data\n",
        "combined_data = []\n",
        "i=0;\n",
        "for npy_file in npy_files:\n",
        "    npy_path = os.path.join(batch_npy_directory, npy_file)\n",
        "    batch_data = np.load(npy_path)\n",
        "    #print(batch_data.shape)\n",
        "    print(npy_path)\n",
        "    f1[i]=batch_data\n",
        "    i=i+1\n",
        "    #cv2_imshow(batch_data[0])\n",
        "    combined_data.append(batch_data)\n",
        "\n",
        "# Concatenate all batch data into a single array\n",
        "combined_data = np.concatenate(combined_data, axis=0)\n",
        "\n",
        "# Save the combined array as a single .npy file\n",
        "#output_npy_filename = os.path.join(output_directory, 'combined_data.npy')\n",
        "#np.save(output_npy_filename, combined_data)\n",
        "\n",
        "print(f1.shape)\n",
        "#print(f1'Combined batch .npy files into a single .npy file.')\n",
        "\n",
        "npy_data=f1\n",
        "print(npy_data.shape)\n",
        "#print(npy_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYk-FD93hIbF"
      },
      "outputs": [],
      "source": [
        "#npy_data = np.expand_dims(npy_data, axis=-1)\n",
        "npy_data = npy_data[:10, ...]\n",
        "\n",
        "\n",
        "# Split into train and validation sets using indexing to optimize memory.\n",
        "indexes = np.arange(npy_data.shape[0])\n",
        "print(indexes)\n",
        "np.random.shuffle(indexes)\n",
        "#print(indexes)\n",
        "train_index = indexes[: int(0.9 * npy_data.shape[0])]\n",
        "val_index = indexes[int(0.9 * npy_data.shape[0]) :]\n",
        "#print(train_index)\n",
        "train_dataset = npy_data[train_index]\n",
        "#print(train_dataset.shape)\n",
        "val_dataset = npy_data[val_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me24hGqahIXn"
      },
      "outputs": [],
      "source": [
        "# Normalize the data to the 0-1 range.\n",
        "train_dataset = train_dataset / 255.0\n",
        "val_dataset = val_dataset / 255.0\n",
        "\n",
        "# We'll define a helper function to shift the frames, where\n",
        "# `x` is frames 0 to n - 1, and `y` is frames 1 to n.\n",
        "def create_shifted_frames(data):\n",
        "    x = data[:, 0 : data.shape[1] - 1, :, :]\n",
        "    y = data[:, 1 : data.shape[1], :, :]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\n",
        "# Apply the processing function to the datasets.\n",
        "x_train, y_train = create_shifted_frames(train_dataset)\n",
        "x_val, y_val = create_shifted_frames(val_dataset)\n",
        "\n",
        "# Inspect the dataset.\n",
        "print(\"Training Dataset Shapes: \" + str(x_train.shape) + \", \" + str(y_train.shape))\n",
        "print(\"Validation Dataset Shapes: \" + str(x_val.shape) + \", \" + str(y_val.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFy3PtTChIVs"
      },
      "outputs": [],
      "source": [
        "# Construct a figure on which we will visualize the images.\n",
        "fig, axes = plt.subplots(4, 5, figsize=(10, 8))\n",
        "\n",
        "# Plot each of the sequential images for one random data example.\n",
        "data_choice = np.random.choice(range(len(train_dataset)), size=1)[0]\n",
        "for idx, ax in enumerate(axes.flat):\n",
        "    ax.imshow(np.squeeze(train_dataset[data_choice][idx]), cmap=\"gray\")\n",
        "    ax.set_title(f\"Frame {idx + 1}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Print information and display the figure.\n",
        "print(f\"Displaying frames for example {data_choice}.\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWKx-jr1hITY"
      },
      "outputs": [],
      "source": [
        "# Construct the input layer with no definite frame size.\n",
        "inp = layers.Input(shape=(None, *x_train.shape[2:]))\n",
        "\n",
        "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
        "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(5, 5),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(inp)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(3, 3),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.ConvLSTM2D(\n",
        "    filters=64,\n",
        "    kernel_size=(1, 1),\n",
        "    padding=\"same\",\n",
        "    return_sequences=True,\n",
        "    activation=\"relu\",\n",
        ")(x)\n",
        "x = layers.Conv3D(\n",
        "    filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        ")(x)\n",
        "\n",
        "# Next, we will build the complete model and compile it.\n",
        "model = keras.models.Model(inp, x)\n",
        "model.compile(\n",
        "    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)"
      ],
      "metadata": {
        "id": "mcC-JdatqaU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEXm-tpVhIRa"
      },
      "outputs": [],
      "source": [
        "# Define some callbacks to improve training.\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
        "\n",
        "# Define modifiable training hyperparameters.\n",
        "epochs = 60\n",
        "batch_size = 4\n",
        "\n",
        "# Fit the model to the training data.\n",
        "model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d6bU4mchIPX"
      },
      "outputs": [],
      "source": [
        "# Select a random example from the validation dataset.\n",
        "example = val_dataset[np.random.choice(range(len(val_dataset)), size=1)[0]]\n",
        "\n",
        "# Pick the first/last ten frames from the example.\n",
        "frames = example[:10, ...]\n",
        "original_frames = example[10:, ...]\n",
        "\n",
        "# Predict a new set of 10 frames.\n",
        "for _ in range(10):\n",
        "    # Extract the model's prediction and post-process it.\n",
        "    new_prediction = model.predict(np.expand_dims(frames, axis=0))\n",
        "    new_prediction = np.squeeze(new_prediction, axis=0)\n",
        "    predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
        "\n",
        "    # Extend the set of prediction frames.\n",
        "    frames = np.concatenate((frames, predicted_frame), axis=0)\n",
        "\n",
        "# Construct a figure for the original and new frames.\n",
        "fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
        "\n",
        "# Plot the original frames.\n",
        "for idx, ax in enumerate(axes[0]):\n",
        "    ax.imshow(np.squeeze(original_frames[idx]), cmap=\"gray\")\n",
        "    ax.set_title(f\"Frame {idx + 11}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Plot the new frames.\n",
        "new_frames = frames[10:, ...]\n",
        "for idx, ax in enumerate(axes[1]):\n",
        "    ax.imshow(np.squeeze(new_frames[idx]), cmap=\"gray\")\n",
        "    ax.set_title(f\"Frame {idx + 11}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Display the figure.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import MeanSquaredError\n",
        "\n",
        "# Instantiate the MeanSquaredError metric\n",
        "mse = MeanSquaredError()\n",
        "\n",
        "# Assuming original_frames is your ground truth and new_frames is the prediction from the model\n",
        "mse.update_state(original_frames, new_frames)\n",
        "mse_result = mse.result().numpy()\n",
        "\n",
        "print(\"Mean Squared Error: \", mse_result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP1pjJWMwAJE",
        "outputId": "c76e67d3-4480-49cd-d176-4b3ba3dcdd92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error:  0.23482886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Assuming original_frames is your ground truth and new_frames is the prediction from the model\n",
        "r2 = r2_score(original_frames.reshape(-1), new_frames.reshape(-1))\n",
        "\n",
        "print(\"R2 Score: \", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESxnRhSLwAku",
        "outputId": "2b5ea2f4-767f-4fee-fe96-392a513e75bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Score:  0.022071291537650084\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}